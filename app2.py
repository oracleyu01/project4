"""
ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘ ì•± - ê°„ë‹¨í•œ ë²„ì „ (LangGraph ì—†ì´)
"""

import streamlit as st
import pandas as pd
from supabase import create_client
from openai import OpenAI
import os
from dotenv import load_dotenv
from datetime import datetime
import time
import json
import re
import requests
from bs4 import BeautifulSoup
import numpy as np

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘",
    page_icon="ğŸ›’",
    layout="wide"
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL") or st.secrets.get("SUPABASE_URL", "")
SUPABASE_KEY = os.getenv("SUPABASE_KEY") or st.secrets.get("SUPABASE_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID") or st.secrets.get("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET") or st.secrets.get("NAVER_CLIENT_SECRET", "")

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .pros-section {
        background-color: #d4edda;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 5px solid #28a745;
    }
    .cons-section {
        background-color: #f8d7da;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 5px solid #dc3545;
    }
</style>
""", unsafe_allow_html=True)

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ›’ ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘</h1>
    <p style="font-size: 1.2rem; margin-top: 1rem;">
        ë¸”ë¡œê·¸ì—ì„œ ìˆ˜ì§‘í•œ ì‹¤ì‚¬ìš© í›„ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œí’ˆì˜ ì¥ì ê³¼ ë‹¨ì ì„ í•œëˆˆì— í™•ì¸í•˜ì„¸ìš”
    </p>
</div>
""", unsafe_allow_html=True)

# OpenAI í´ë¼ì´ì–¸íŠ¸
@st.cache_resource
def get_openai_client():
    return OpenAI(api_key=OPENAI_API_KEY)

# Supabase í´ë¼ì´ì–¸íŠ¸
@st.cache_resource
def get_supabase_client():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰
def search_naver_blog(query, display=10):
    url = "https://openapi.naver.com/v1/search/blog"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {"query": query, "display": display, "sort": "sim"}
    
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            return response.json()
    except:
        pass
    return None

# HTML íƒœê·¸ ì œê±°
def remove_html_tags(text):
    text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub(r'<[^>]+>', '', text)
    return text.strip()

# ë¸”ë¡œê·¸ ë‚´ìš© í¬ë¡¤ë§
def crawl_blog_content(url):
    try:
        if "blog.naver.com" in url:
            parts = url.split('/')
            if len(parts) >= 5:
                blog_id = parts[3]
                post_no = parts[4].split('?')[0]
                mobile_url = f"https://m.blog.naver.com/{blog_id}/{post_no}"
                
                response = requests.get(mobile_url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    for selector in ['div.se-main-container', 'div#postViewArea', 'div.post_ct']:
                        elem = soup.select_one(selector)
                        if elem:
                            content = elem.get_text(separator='\n', strip=True)
                            content = re.sub(r'\s+', ' ', content)
                            return content if len(content) > 300 else None
    except:
        pass
    return None

# GPTë¡œ ì¥ë‹¨ì  ì¶”ì¶œ
def extract_pros_cons(product_name, content):
    if not content or len(content) < 200:
        return None
    
    client = get_openai_client()
    prompt = f"""ë‹¤ìŒì€ "{product_name}"ì— ëŒ€í•œ ë¸”ë¡œê·¸ ë¦¬ë·°ì…ë‹ˆë‹¤.

[ë¸”ë¡œê·¸ ë‚´ìš©]
{content[:1500]}

ìœ„ ë‚´ìš©ì—ì„œ {product_name}ì˜ ì¥ì ê³¼ ë‹¨ì ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì¥ì :
- (êµ¬ì²´ì ì¸ ì¥ì )

ë‹¨ì :
- (êµ¬ì²´ì ì¸ ë‹¨ì )

ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì •ë³´ ë¶€ì¡±"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”."""
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë…¸íŠ¸ë¶ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=500
        )
        
        result = response.choices[0].message.content.strip()
        
        if result and "ì •ë³´ ë¶€ì¡±" not in result:
            pros = []
            cons = []
            
            lines = result.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                if 'ì¥ì :' in line:
                    current_section = 'pros'
                elif 'ë‹¨ì :' in line:
                    current_section = 'cons'
                elif line.startswith('-') and current_section:
                    point = line[1:].strip()
                    if point and len(point) > 5:
                        if current_section == 'pros':
                            pros.append(point)
                        else:
                            cons.append(point)
            
            if pros or cons:
                return {'pros': pros[:5], 'cons': cons[:5]}
    except:
        pass
    return None

# ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜
def search_product(product_name):
    supabase = get_supabase_client()
    
    # 1. DBì—ì„œ ê²€ìƒ‰
    try:
        # ì •í™•í•œ ë§¤ì¹­
        result = supabase.table('laptop_pros_cons').select("*").eq('product_name', product_name).execute()
        if result.data:
            return process_db_results(result.data), "database", []
        
        # ë¶€ë¶„ ë§¤ì¹­
        result = supabase.table('laptop_pros_cons').select("*").ilike('product_name', f'%{product_name}%').execute()
        if result.data:
            return process_db_results(result.data), "database", []
    except:
        pass
    
    # 2. ì›¹ì—ì„œ í¬ë¡¤ë§
    all_pros = []
    all_cons = []
    sources = []
    
    search_queries = [
        f"{product_name} ì¥ë‹¨ì  ì‹¤ì‚¬ìš©",
        f"{product_name} í›„ê¸°"
    ]
    
    for query in search_queries[:2]:
        blog_result = search_naver_blog(query, display=5)
        if not blog_result or 'items' not in blog_result:
            continue
        
        for post in blog_result['items'][:3]:
            post['title'] = remove_html_tags(post['title'])
            content = crawl_blog_content(post['link'])
            
            if content:
                pros_cons = extract_pros_cons(product_name, content)
                if pros_cons:
                    all_pros.extend(pros_cons['pros'])
                    all_cons.extend(pros_cons['cons'])
                    sources.append({
                        'title': post['title'],
                        'link': post['link']
                    })
            
            time.sleep(0.5)
    
    # ì¤‘ë³µ ì œê±°
    pros = list(dict.fromkeys(all_pros))[:10]
    cons = list(dict.fromkeys(all_cons))[:10]
    
    # DBì— ì €ì¥
    if pros or cons:
        try:
            data = []
            for pro in pros:
                data.append({
                    'product_name': product_name,
                    'type': 'pro',
                    'content': pro
                })
            for con in cons:
                data.append({
                    'product_name': product_name,
                    'type': 'con',
                    'content': con
                })
            supabase.table('laptop_pros_cons').insert(data).execute()
        except:
            pass
    
    return {'pros': pros, 'cons': cons}, "web_crawling", sources[:5]

def process_db_results(data):
    pros = [item['content'] for item in data if item['type'] == 'pro']
    cons = [item['content'] for item in data if item['type'] == 'con']
    return {'pros': pros, 'cons': cons}

# ê²€ìƒ‰ UI
col1, col2, col3 = st.columns([1, 3, 1])

with col2:
    product_name = st.text_input(
        "ğŸ” ì œí’ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš”",
        placeholder="ì˜ˆ: ë§¥ë¶ í”„ë¡œ M3, LG ê·¸ë¨ 2024, ê°¤ëŸ­ì‹œë¶4 í”„ë¡œ"
    )
    
    search_button = st.button("ê²€ìƒ‰í•˜ê¸°", use_container_width=True)

# ê²€ìƒ‰ ì‹¤í–‰
if search_button and product_name:
    with st.spinner(f"'{product_name}' ê²€ìƒ‰ ì¤‘..."):
        results, method, sources = search_product(product_name)
    
    if results['pros'] or results['cons']:
        # ê²€ìƒ‰ ë°©ë²• í‘œì‹œ
        if method == "database":
            st.success(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}' ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
        else:
            st.warning(f"ğŸ”„ ì›¹ì—ì„œ '{product_name}' ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
        
        # ì¥ë‹¨ì  í‘œì‹œ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            <div class="pros-section">
                <h3>âœ… ì¥ì </h3>
            </div>
            """, unsafe_allow_html=True)
            
            for idx, pro in enumerate(results['pros'], 1):
                st.write(f"{idx}. {pro}")
        
        with col2:
            st.markdown("""
            <div class="cons-section">
                <h3>âŒ ë‹¨ì </h3>
            </div>
            """, unsafe_allow_html=True)
            
            for idx, con in enumerate(results['cons'], 1):
                st.write(f"{idx}. {con}")
        
        # í†µê³„
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ì¥ì ", f"{len(results['pros'])}ê°œ")
        with col2:
            st.metric("ì´ ë‹¨ì ", f"{len(results['cons'])}ê°œ")
        with col3:
            st.metric("ê²€ìƒ‰ ë°©ë²•", "DB" if method == "database" else "ì›¹")
        
        # ì¶œì²˜
        if sources:
            with st.expander("ğŸ“š ì¶œì²˜ ë³´ê¸°"):
                for idx, source in enumerate(sources, 1):
                    st.write(f"{idx}. [{source['title']}]({source['link']})")
    else:
        st.error(f"'{product_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
current_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
st.markdown(f"""
<div style="text-align: center; color: #666; padding: 2rem;">
    <p>ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {current_date}</p>
    <p>Powered by OpenAI</p>
</div>
""", unsafe_allow_html=True)
